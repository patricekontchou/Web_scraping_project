{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to Processed the two datasets collected using scrapy and selenium(gdoor.csv and sector.csv)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning file containaing company, industry type and sector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.DataFrame(pd.read_csv('./Projects/Web_scraping/gdoor.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns',100)\n",
    "pd.set_option('max_rows',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('./raw_data/sector.csv', sep='\\t', header = None)\n",
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.columns = ['company', 'address','size','year_founded', 'type', 'industry','sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f.drop(f.company[f.company.map(lambda x: type(x) is float)].index,0)\n",
    "\n",
    "f = f.drop(f[f.industry.map(lambda x: type(x) is float)].index,0)\n",
    "\n",
    "f.sector[f.sector.map(lambda x: type(x) is float)] = 'Unkown'\n",
    "\n",
    "f.sector[f.sector.str.contains('Unknown')] = f.industry[f.sector.str.contains('Unknown')]\n",
    "\n",
    "f = f.applymap(lambda x: x.strip())   #or use df.transform(lambda x: x.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning file containing list of jobs and respective companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = pd.read_csv('./raw_data/gdoor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2= f2.loc[:,['company','days_posted','job_title','location','max_salary','start_salary','rating']]\n",
    "\n",
    "f2.loc[:,['company','days_posted','job_title']] \\\n",
    "= f2.loc[:,['company','days_posted','job_title']].applymap(lambda x: x.strip())\n",
    "\n",
    "f2.company = f2.company.map(lambda x: x.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and normalize posted_days \n",
    "\n",
    "def days_posted_extract(x):\n",
    "    if re.findall('[a-z]+',x)[0] == 'hr':\n",
    "       val = round(int(re.findall('\\d+',x)[0])/24)\n",
    "    elif re.findall('[a-z]+',x)[0] == 'd':\n",
    "        val = int(re.findall('\\d+',x)[0])\n",
    "    return val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Salary and normalized\n",
    "\n",
    "def extract_salary(x):\n",
    "    if type(x) is float:\n",
    "        val = 0\n",
    "    elif type(x) is str:\n",
    "        val = int(re.findall('\\d+',x)[0])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2['days_posted_c'] = f2['days_posted'].map(days_posted_extract)\n",
    "\n",
    "salary = f2.loc[:,['start_salary', 'max_salary']].applymap(extract_salary)\n",
    "\n",
    "f2.rename(columns = {'start_salary': 'start_salary_old' , 'max_salary': 'max_salary_old'}, inplace=True)\n",
    "\n",
    "f2 = pd.concat([f2,salary],axis ='columns')\n",
    "\n",
    "#convert rating to whole number by rounding \n",
    "f2['rating_c'] = f2.rating.map(lambda x: 0 if pd.isnull(x)   else round(x))\n",
    "\n",
    "#f2 =f2.reindex(f2.columns.tolist() + ['size','year_founded','type','industry','sector'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(f2,f, how='outer', left_on=['company','location'], right_on=['company','address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract only columns needed for analysis\n",
    "merge_df = merge_df.loc[:,['company', 'location','job_title', 'days_posted_c', 'start_salary','max_salary','rating_c',\\\n",
    "                           'address','size','year_founded', 'type', 'industry',\\\n",
    "                           'sector']].drop_duplicates().sort_values('company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard data points with invalid address\n",
    "merge_df.reset_index(inplace=True)\n",
    "merge_df.drop(['index'],axis=1, inplace=True)\n",
    "merge_df = merge_df[merge_df.location.isnull() == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split address to City and Sate\n",
    "merge_df['city'] = merge_df.location.map(lambda x: x.split(',')[0] if len(x.split(',')) > 1 else 'Statewide')\n",
    "merge_df['state']  = merge_df.location.map(lambda x: x.split(',')[1] if len(x.split(',')) > 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change State names to Abbrev in some data points\n",
    "merge_df.state[merge_df.state == 'Oregon'] = 'OR'\n",
    "merge_df.state[merge_df.state == 'Maryland'] = 'MD'\n",
    "merge_df.state[merge_df.state == 'Virginia'] = 'VA'\n",
    "merge_df.state[merge_df.state == 'Nevada'] = 'NV'\n",
    "merge_df.state[merge_df.state == 'Nebraska'] = 'NE'\n",
    "merge_df.state[merge_df.state == 'Georgia'] = 'GA'\n",
    "merge_df.state[merge_df.state == 'Wisconsin'] = 'WI'\n",
    "merge_df.state[merge_df.state == 'New Jersey'] = 'NJ'\n",
    "merge_df.state[merge_df.state == 'Illinois'] = 'IL'\n",
    "merge_df.state[merge_df.state == 'New York State'] = 'NY'\n",
    "merge_df.state[merge_df.state == 'Utah'] = 'UT'\n",
    "merge_df.state[merge_df.state == 'Remote'] = 'US'\n",
    "merge_df.state[merge_df.state == 'United States'] = 'US'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save full processed data\n",
    "merge_df.to_csv('./clean_data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a copy of data with only columns needed for analysis\n",
    "clean_df = merge_df.loc[:,['company','city','state','year_founded','size','rating_c','job_title','days_posted_c',\\\n",
    "             'start_salary','max_salary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save clean data\n",
    "clean_df.to_csv('./clean_data/clean_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
